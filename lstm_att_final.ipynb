{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from netCDF4 import Dataset as ncread\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from cftime import DatetimeNoLeap\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import get_principle_components_and_EOFs\n",
    "import os\n",
    "import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "import cmocean #A package with beautiful colormaps\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = '/data/volume_2/observational/raw/'\n",
    "\n",
    "import pathlib\n",
    "root_results = str(pathlib.Path.home() / 'Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### target \n",
    "see https://github.com/AI4S2S/Lorentz_s2spy_workshop/blob/main/preprocess_target.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYY = 1981   # start year, could be changed\n",
    "EYY = 2021   # end year, could be changed\n",
    "\n",
    "\n",
    "drop_OND_years = [2005,2007,2018,2004,2006]\n",
    "take_OND_years = list(np.arange(SYY,EYY+1))\n",
    "take_OND_years = [y for y in take_OND_years if y not in drop_OND_years]\n",
    "# drop_MAM_years = [2009,2001,2002,2005,2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left aligned\n",
    "t = xr.open_dataset(\"/data/volume_2/observational/chrips_1981-2021_target_new_left.nc\").sel(time = slice(str(SYY),str(EYY)))\n",
    "# take only OND: oct 1st - Dec 3rd\n",
    "t = xr.concat([t.sel(time=slice(f\"{yyyy}-10-01\",f\"{yyyy}-12-03\")) for yyyy in t.time.dt.year.to_index().unique()],\"time\")\n",
    "\n",
    "tp_index = t.binary # 1 drought, 0 no drought: reversed to initial commits\n",
    "\n",
    "tp_target = t.tp_28d_rm # 28 day rolling mean rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare predictors\n",
    "- 28-days time series with the last day two weeks before the target day\n",
    "- dimensionality reduction: EOFs\n",
    "- each variable has their own region for EOF extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor data preprocessing\n",
    "# can select the values and region you want by changing the parameters\n",
    "\n",
    "file_vars = ['ERA5_t2m', 'era5_t_850hpa', 'era5_z_200hpa', 'era5_z_500hpa', 'sst', 'era5_olr']\n",
    "#file_vars = ['sst']\n",
    "header_vars = ['t2m', 't', 'z', 'z', 'sst', 'olr-mean']\n",
    "#header_vars = ['sst']\n",
    "\n",
    "# select regions for the individual predictor\n",
    "lon_slices = [[-16,54],[-30,90],[-30,90],[-30,90],[-180,180],[40,180]]\n",
    "lat_slices = [[16,0],[30,-20],[-20,30],[-20,30],[40,-20],[-20,20]]\n",
    "\n",
    "nmode = 5 # for eofs\n",
    "\n",
    "for file_var, header_var, lon_slice, lat_slice in zip (file_vars, header_vars, lon_slices, lat_slices):\n",
    "    # use existing\n",
    "    path = root_results+'/PC_series_n_'+str(nmode)+'_var_'+file_var+'_.nc'\n",
    "    path_eof = root_results+'/EOF_maps_n_'+str(nmode)+'_var_'+file_var+'_.nc'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        continue\n",
    "    if file_var == 'era5_olr':\n",
    "        file = xr.open_dataset(root_data+file_var+'_1950_2021_daily_1deg_tropics.nc')\n",
    "        print('olr')\n",
    "    else:\n",
    "        file = xr.open_dataset(root_data+file_var+'_1959-2021_1_12_daily_2.0deg.nc')\n",
    "        print(header_var)\n",
    "\n",
    "    if \"longitude\" in file.coords:\n",
    "        file = file.rename({\"longitude\": \"lon\",\"latitude\": \"lat\"})\n",
    "\n",
    "    assert \"lat\" in file.coords\n",
    "    assert \"lon\" in file.coords\n",
    "    \n",
    "    # select region\n",
    "    var_dim = file.sel(lon=slice(lon_slice[0],lon_slice[1]),lat=slice(lat_slice[0],lat_slice[1]))\n",
    "    \n",
    "    # todo: train_valid_test_split: exclude test    \n",
    "    # take years 1980 - 2021 daily and only and 7 day rolling mean\n",
    "    var_series = var_dim.sel(time=var_dim.time.dt.year.isin([np.arange(SYY,EYY+1)])).rolling(time=7, center=False).mean(skipna=True)\n",
    "    var_series = var_series.sel(time=var_series.time.dt.year.isin(take_OND_years))\n",
    "\n",
    "    # remove climatology\n",
    "    var_anom_series = var_series.groupby(\"time.dayofyear\") - var_series.groupby(\"time.dayofyear\").mean(\"time\",skipna=True)\n",
    "    \n",
    "    # use the months you want (base on how long the time series used as predictors)\n",
    "    var_anom_sel = var_anom_series.sel(time=var_anom_series.time.dt.month.isin([7,8,9,10,11]))[header_var]\n",
    "    \n",
    "    # Apply EOF\n",
    "    if file_var == \"era5_z_500hpa\":\n",
    "        header_var = \"z500\"\n",
    "    pc_xr, EOF = get_principle_components_and_EOFs(var_anom_sel, nmode=nmode)\n",
    "    \n",
    "    pc_xr = pc_xr.assign_coords(mode=[str(header_var)+'_'+str(int(m)) for m in pc_xr.mode])\n",
    "    EOF = EOF.assign_coords(mode=[str(header_var)+'_'+str(int(m)) for m in EOF.mode])\n",
    "    # save to disk\n",
    "    pc_xr.to_netcdf(path)\n",
    "    EOF.to_netcdf(path_eof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick viz\n",
    "# pc_xr.plot(hue=\"mode\", figsize=(20,3))\n",
    "# var_anom_series[header_var].isel(time=122).plot(size=5, aspect=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exec(open('/home/mpyrina/Notebooks/Lorentz_workshop/L_functions.py').read())\n",
    "\n",
    "from L_functions import sel_train_data_lead\n",
    "\n",
    "#Create predictor multi-file\n",
    "nc_in_file='PC_serie*.nc'\n",
    "dim_to_stack='mode'\n",
    "pc_xr = xr.open_mfdataset(root_results+\"/\"+nc_in_file,concat_dim=dim_to_stack,\n",
    "                          combine=\"nested\")\n",
    "\n",
    "# Run the function\n",
    "s_target_date='01-10-1980'\n",
    "e_target_date='03-12-2021' # end of year - 28 days\n",
    "rw_1 = 7\n",
    "lead_time = 15 # days until valid_time starts\n",
    "rw = 0 # because the data are not centered\n",
    "ntimestep = 60 # lags\n",
    "target_len = len(tp_target)\n",
    "\n",
    "predictor_array=sel_train_data_lead(pc_xr, target_len, s_target_date, e_target_date,\n",
    "                rw_1, lead_time, rw, ntimestep)\n",
    "\n",
    "np_out_name='Predictor_array_crosscor_number.nc'\n",
    "\n",
    "#np.save(root_results+np_out_name,predictor_array)\n",
    "predictor_array.to_netcdf(root_results+\"/\"+np_out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard fix dates intersection\n",
    "dates = tp_index.time.to_index().intersection(predictor_array.time.to_index())\n",
    "\n",
    "tp_index = tp_index.sel(time=dates).compute()\n",
    "tp_target = tp_target.sel(time=dates).compute()\n",
    "predictor_array = predictor_array.sel(time=dates).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## causal discovery with `tigramite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# install with pip install tigramite\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, CMIknn, GPDC\n",
    "\n",
    "# Example data\n",
    "features_train = predictor_array.sel(lag=0).pcs\n",
    "sample_size = features_train.time.size\n",
    "N_features = features_train.mode.size\n",
    "allX = features_train.values \n",
    "\n",
    "# Target\n",
    "y = tp_target.values\n",
    "\n",
    "# Construct array needed for tigramite, we need to lag X behind y here just for computation reasons\n",
    "data = np.hstack((y[:-1].reshape(sample_size-1, 1), allX[1:]))\n",
    "\n",
    "# Initialize class with ParCorr test, can be changed to nonlinear CI tests, eg CMIknn, but these use more computation time\n",
    "dataframe = pp.DataFrame(data, var_names = ['target',] + list(range(N_features)))\n",
    "pcmci = PCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=ParCorr(),  # or CMIknn()  GPDC()\n",
    "    verbosity=0)\n",
    "\n",
    "# Set alpha_level for selecting causal features, the smaller the stricter\n",
    "pc_alpha = 0.01\n",
    "\n",
    "# Only run on target variable\n",
    "selected_links = [(i, -1) for i in range(1, N_features + 1)]\n",
    "causal_predictors = pcmci._run_pc_stable_single(j=0,\n",
    "                              selected_links=selected_links,\n",
    "                              tau_min=1,\n",
    "                              tau_max=1,\n",
    "                              pc_alpha=pc_alpha)\n",
    "\n",
    "# Indices of causal features from X\n",
    "causal_features = [varlag[0] - 1 for varlag in causal_predictors['parents']]\n",
    "print(causal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could subselect these later\n",
    "predictor_array.isel(mode=causal_features).mode.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eofs = xr.open_mfdataset(root_results+\"/EOF_*\",concat_dim=dim_to_stack,\n",
    "                          combine=\"nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for m in causal_features:\n",
    "#    eofs.isel(mode=m).eofs.plot(robust=True)\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_OND_years = [2005,2007,2018,2004,2006]\n",
    "drop_MAM_years = [2009,2001,2002,2005,2020]\n",
    "\n",
    "def get_train_test_val(data_predictor, data_target, test_frac, val_frac):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last int(-test_frac*len(tp_predictor)) rows to test data\n",
    "    test_predictor = data_predictor[int(-test_frac*len(data_target)):]\n",
    "    test_target = data_target[int(-test_frac*len(data_target)):]\n",
    "    \n",
    "    # assign the last int(-test_frac*len(tp_predictor)) from the remaining rows to validation data\n",
    "    remain_predictor = data_predictor[0:int(-test_frac*len(data_target))]\n",
    "    remain_target = data_target[0:int(-test_frac*len(data_target))]\n",
    "    val_predictor = remain_predictor[int(-val_frac*len(remain_predictor)):]\n",
    "    val_target = remain_target[int(-val_frac*len(remain_predictor)):]\n",
    "    \n",
    "    # the remaining rows are assigned to train data\n",
    "    train_predictor = remain_predictor[:int(-val_frac*len(remain_predictor))]\n",
    "    train_target = remain_target[:int(-val_frac*len(remain_predictor))]\n",
    "    return train_predictor, train_target, test_predictor, test_target, val_predictor, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output data for LSTM\n",
    "y_all = keras.utils.to_categorical(tp_index)\n",
    "X_all = predictor_array.pcs\n",
    "print(X_all.shape,y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y, val_X, val_y = get_train_test_val(X_all, y_all, test_frac=0.2, val_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_split_counts(train_y, val_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM with attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/gentaiscool/lstm-attention/blob/58adc7e345b5b3a79638483049704802a66aa1f4/layers.py#L50\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    follows these equations:\n",
    "    \n",
    "    (1) u_t = tanh(W h_t + b)\n",
    "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "    (3) v_t = \\alpha_t * h_t, v in time t\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'W_regularizer': self.W_regularizer,\n",
    "                'u_regularizer': self.u_regularizer,\n",
    "                'b_regularizer': self.b_regularizer,\n",
    "                'W_constraint': self.W_constraint,\n",
    "                'u_constraint': self.u_constraint,\n",
    "                'b_constraint': self.b_constraint,\n",
    "                'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer='zero',\n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "        # Should add a small epsilon as the workaround\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return weighted_input, a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "    \n",
    "class Addition(Layer):\n",
    "    \"\"\"\n",
    "    This layer is supposed to add of all activation weight.\n",
    "    We split this from AttentionWithContext to help us getting the activation weights\n",
    "    follows this equation:\n",
    "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
    "    \n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class weight dictionary to help if the classes are unbalanced\n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    weights = np.max(np.sum(Y, axis=0)) / np.sum(Y, axis=0)\n",
    "    for i in range( Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = class_weight_creator(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "shuffle = True \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_path = '/home/zwu/Lorentz_workshop/test/checkpoint_test'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=callbacks_path,\n",
    "        monitor='val_acc',   # tf.keras.metrics.AUC(from_logits=True)\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with attention layer\n",
    "ntimestep = 60    # number of time step used in the predictors\n",
    "nfeature = 30   # number of features\n",
    "input_tensor = Input(shape=(ntimestep,nfeature))\n",
    "layer1 = layers.LSTM(100, return_sequences=True, kernel_regularizer=regularizers.l2(1))(input_tensor)\n",
    "layer1 = layers.LSTM(20, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(layer1)\n",
    "layer1, alfa = AttentionWithContext()(layer1)\n",
    "layer1 = Addition()(layer1)\n",
    "layer1 = layers.Dense(5, activation=\"relu\")(layer1)\n",
    "output_tensor = layers.Dense(2,activation='softmax')(layer1)\n",
    "\n",
    "model = Model(input_tensor, output_tensor)\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_X, val_y), shuffle = shuffle, verbose=verbose, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0,1.1)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_loss, label='Training loss')\n",
    "ax2.plot(val_loss, label='Validation loss')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "visualization.plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "test_predict = model.predict(test_X)\n",
    "y_pred = np.argmax(model.predict(test_X),axis=1)\n",
    "print('Recall: '+str(round(recall_score(test_y[:,1],y_pred),2)))\n",
    "print('Precision: '+str(round(precision_score(test_y[:,1],y_pred),2)))\n",
    "print('F1-score: '+str(round(f1_score(test_y[:,1],y_pred),2)))\n",
    "print('Accuracy: '+str(round(accuracy_score(test_y[:,1],y_pred),2)))\n",
    "print('Brier score:' +str(brier_score_loss(test_y[0:-20,1], test_predict[0:-20,1])))\n",
    "\n",
    "calib_y, calib_x = calibration_curve(test_y[:,1],test_predict[:,1],n_bins=10)\n",
    "visualization.plot_calibration_curve(calib_x, calib_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_roc_auc(model, test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weightings of each time step and each sample\n",
    "intermediate_layer_model2 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[3].output)\n",
    "\n",
    "intermediate_layer_model1 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[2].output)\n",
    "\n",
    "intermediate_layer_model3 = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[4].output)\n",
    "\n",
    "intermediate_output2, alfa_output = intermediate_layer_model2.predict(test_X, verbose=0)\n",
    "intermediate_output1 = intermediate_layer_model1.predict(test_X, verbose=0)\n",
    "intermediate_output3 = intermediate_layer_model3.predict(test_X, verbose=0)\n",
    "\n",
    "weights = intermediate_output2 / intermediate_output1\n",
    "print(np.shape(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "val_weights = np.ndarray((len(test_X),ntimestep))+np.nan\n",
    "for ii in range(len(test_X)):\n",
    "    for j in range(ntimestep):\n",
    "        val_weights[ii,j] = weights[ii][j][0]\n",
    "print(np.shape(val_weights))\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "for ii in range(len(test_X)):\n",
    "    plt.plot(val_weights[ii,:])\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=plt.figaspect(0.15))\n",
    "plt.plot(np.nanmean(val_weights,axis=0),'k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
